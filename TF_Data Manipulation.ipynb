{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f35f1c",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55f70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os \n",
    "import time\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f63291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the longitude from 0 to 360 to -180 to 180\n",
    "def adjust_longitude(lon):\n",
    "    lon_adjusted = ((lon + 180) % 360) - 180\n",
    "    lon_adjusted[lon_adjusted == -180] = 180\n",
    "    return lon_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15cf0a",
   "metadata": {},
   "source": [
    "## ERA5-Land 2m Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac51226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/load dataset\n",
    "path_era5 = '/Users/alexaterrazas/Desktop/Lake Transfer Function Manuscript/ERA5 data/'\n",
    "ds_tas = xr.open_dataset(path_era5 + 'tas_data.nc')\n",
    "\n",
    "# Subset the data for the time range from 2013 to 2021\n",
    "tas = ds_tas['t2m'].sel(time=slice('2013-01-01', '2021-12-01'))\n",
    "\n",
    "# Calculate the monthly mean\n",
    "tas_monthly_mean = tas.groupby('time.month').mean(dim='time')\n",
    "\n",
    "# Convert from Kelvin to Celsius\n",
    "tas_monthly_mean = tas_monthly_mean - 273.15\n",
    "\n",
    "# Apply the adjust_longitude function\n",
    "tas_monthly_mean.coords['longitude'] = adjust_longitude(tas_monthly_mean.coords['longitude'])\n",
    "\n",
    "# Sort by longitude to ensure the dataset is correctly ordered\n",
    "tas_monthly_mean = tas_monthly_mean.sortby('longitude')\n",
    "\n",
    "tas_monthly_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e271ec9",
   "metadata": {},
   "source": [
    "## ERA5 Total Cloud Cover Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d989932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/load dataset\n",
    "path_era5 = '/Users/alexaterrazas/Desktop/Lake Transfer Function Manuscript/ERA5 data/'\n",
    "ds_tcc = xr.open_dataset(path_era5 + 'tcc_data.nc')\n",
    "\n",
    "# Subset the data for the time range from 2013 to 2021\n",
    "tcc = ds_tcc['tcc'].sel(time=slice('2013-01-01', '2021-12-01'))\n",
    "\n",
    "# Convert to percentage\n",
    "tcc_percentage = tcc * 100\n",
    "\n",
    "# Calculate the monthly mean\n",
    "tcc_monthly_mean = tcc_percentage.groupby('time.month').mean(dim='time')\n",
    "\n",
    "# Apply the adjust_longitude function\n",
    "tcc_monthly_mean.coords['longitude'] = adjust_longitude(tcc_monthly_mean.coords['longitude'])\n",
    "\n",
    "# Sort by longitude to ensure the dataset is correctly ordered\n",
    "tcc_monthly_mean = tcc_monthly_mean.sortby('longitude')\n",
    "\n",
    "\n",
    "tcc_monthly_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ef34c",
   "metadata": {},
   "source": [
    "## ERA5-Land 2m Dewpoint Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e62c6",
   "metadata": {},
   "source": [
    "### Dew Point Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/load dataset\n",
    "path_era5 = '/Users/alexaterrazas/Desktop/Lake Transfer Function Manuscript/ERA5 data/'\n",
    "ds_d2m_sp = xr.open_dataset(path_era5 + 'd2m_sp_data.nc')\n",
    "\n",
    "# Subset the data for the time range from 2013 to 2021\n",
    "d2m = ds_d2m_sp['d2m'].sel(time=slice('2013-01-01', '2021-12-01'))\n",
    "\n",
    "# Calculate the monthly mean\n",
    "d2m_monthly_mean = d2m.groupby('time.month').mean(dim='time')\n",
    "\n",
    "# Convert from Kelvin to Celsius\n",
    "d2m_monthly_mean = d2m_monthly_mean - 273.15\n",
    "\n",
    "# Convert from Kelvin to Celsius\n",
    "d2m_monthly_mean = d2m_monthly_mean\n",
    "\n",
    "# Apply the adjust_longitude function\n",
    "d2m_monthly_mean.coords['longitude'] = adjust_longitude(d2m_monthly_mean.coords['longitude'])\n",
    "\n",
    "# Sort by longitude to ensure the dataset is correctly ordered\n",
    "d2m_monthly_mean = d2m_monthly_mean.sortby('longitude')\n",
    "d2m_monthly_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f175d9",
   "metadata": {},
   "source": [
    "## Calculate Relative Humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81930fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saturation_vapor_pressure(T):\n",
    "    \"\"\"\n",
    "    Calculate the saturation vapor pressure using the Clausius-Clapeyron equation.\n",
    "    \n",
    "    Parameters:\n",
    "    T (float): Temperature in Kelvin.\n",
    "    \n",
    "    Returns:\n",
    "    float: Saturation vapor pressure in hPa.\n",
    "    \"\"\"\n",
    "    e0 = 6.112  # hPa\n",
    "    Lv = 2.5e6  # J/kg\n",
    "    Rv = 461.5  # J/(kgÂ·K)\n",
    "    T0 = 273.15  # K\n",
    "    \n",
    "    es = e0 * np.exp((Lv / Rv) * (1 / T0 - 1 / T))\n",
    "    \n",
    "    return es\n",
    "\n",
    "def calculate_relative_humidity(T, Td):\n",
    "    \"\"\"\n",
    "    Calculate the relative humidity given temperature (T) and dewpoint temperature (Td).\n",
    "    \n",
    "    Parameters:\n",
    "    T (float): Temperature in degrees Celsius.\n",
    "    Td (float): Dewpoint temperature in degrees Celsius.\n",
    "    \n",
    "    Returns:\n",
    "    float: Relative humidity as a percentage.\n",
    "    \"\"\"\n",
    "    # Convert temperatures from Celsius to Kelvin\n",
    "    T_kelvin = T + 273.15\n",
    "    Td_kelvin = Td + 273.15\n",
    "    \n",
    "    # Calculate saturation vapor pressures\n",
    "    es_T = saturation_vapor_pressure(T_kelvin)\n",
    "    es_Td = saturation_vapor_pressure(Td_kelvin)\n",
    "    \n",
    "    # Calculate relative humidity\n",
    "    RH = 100 * (es_Td / es_T)\n",
    "    \n",
    "    return RH\n",
    "\n",
    "# Use calculate_relative_humidity function\n",
    "T = tas_monthly_mean    # Temperature in degrees Celsius\n",
    "Td = d2m_monthly_mean   # Dewpoint temperature in degrees Celsius\n",
    "\n",
    "# Use calculate_relative_humidity function\n",
    "RH_monthly_mean = calculate_relative_humidity(T, Td)\n",
    "RH_monthly_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ca899",
   "metadata": {},
   "source": [
    "## ERA5-Land Surface net solar radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ccaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/load dataset\n",
    "path_era5 = '/Users/alexaterrazas/Desktop/Lake Transfer Function Manuscript/ERA5 data/'\n",
    "ds_ssr = xr.open_dataset(path_era5 + 'ssr_data.nc')\n",
    "\n",
    "# Subset the data for the time range from 2013 to 2021\n",
    "ssr = ds_ssr['ssr'].sel(time=slice('2013-01-01', '2021-12-01'))\n",
    "\n",
    "# Calculate the monthly mean\n",
    "ssr_monthly_mean = ssr.groupby('time.month').mean(dim='time')\n",
    "\n",
    "\n",
    "# Apply the adjust_longitude function\n",
    "ssr_monthly_mean.coords['longitude'] = adjust_longitude(ssr_monthly_mean.coords['longitude'])\n",
    "\n",
    "# Sort by longitude to ensure the dataset is correctly ordered\n",
    "ssr_monthly_mean = ssr_monthly_mean.sortby('longitude')\n",
    "\n",
    "ssr_monthly_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34fc73",
   "metadata": {},
   "source": [
    "## ERA5-Land 10m horizontal wind component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/load dataset\n",
    "path_era5 = '/Users/alexaterrazas/Desktop/Lake Transfer Function Manuscript/ERA5 data/'\n",
    "ds_u10 = xr.open_dataset(path_era5 + 'u10_data.nc')\n",
    "\n",
    "# Subset the data for the time range from 2013 to 2021\n",
    "u10 = ds_u10['u10'].sel(time=slice('2013-01-01', '2021-12-01'))\n",
    "\n",
    "# Calculate the monthly mean\n",
    "u10_monthly_mean = u10.groupby('time.month').mean(dim='time')\n",
    "\n",
    "# Apply the adjust_longitude function\n",
    "u10_monthly_mean.coords['longitude'] = adjust_longitude(u10_monthly_mean.coords['longitude'])\n",
    "\n",
    "# Sort by longitude to ensure the dataset is correctly ordered\n",
    "u10_monthly_mean = u10_monthly_mean.sortby('longitude')\n",
    "\n",
    "u10_monthly_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835e5f5",
   "metadata": {},
   "source": [
    "## LakeTemp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open dataset\n",
    "path_laketemp = '/Users/alexaterrazas/Desktop/Lake Transfer Function Manuscript/LakeTEMP_v1/'\n",
    "lake_data_agg = pd.read_csv(path_laketemp + 'LakeTEMP_aggregated_v1.csv')\n",
    "lake_data_agg\n",
    "\n",
    "#remove reservoirs\n",
    "    #1 = lake\n",
    "    #2 = reservoir\n",
    "    #3 = controlled lake (natural lake with regulation structure)\n",
    "mask_nonlake = lake_data_agg['Lake_type'].isin([1])\n",
    "lake_data = lake_data_agg[mask_nonlake]\n",
    "print(len(lake_data_agg), len(lake_data))\n",
    "\n",
    "#remove uncertain or unreliable data\n",
    "    # relaiable and acceptable >= 46 observations\n",
    "mask_obs = lake_data['n_obs'] >= 46\n",
    "lake_data = lake_data[mask_obs]\n",
    "print(len(lake_data))\n",
    "\n",
    "\n",
    "#assess intermittency and remove lakes that are permanently dry\n",
    "    #3 = 100 % âlandâ + âunknownâ observations (Permanently dry lake with seasonal snow or ice cover)\n",
    "    #4 = 100 % âlandâ observations             (Permanently dry lake; or wetland with dense vegetation)\n",
    "mask_percentland = lake_data['intermittency'].isin([0,1,2])\n",
    "lake_data = lake_data[mask_percentland]\n",
    "\n",
    "#lake surface area can't be smaller than the resolution of the dataset\n",
    "lake_data = lake_data[lake_data['Lake_area'] > 81]\n",
    "lake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e940ccd",
   "metadata": {},
   "source": [
    "## Caculate seasonal lake water temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa50144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate seasonal average based on hemisphere\n",
    "def calculate_seasonal_avg(df, nh_months, sh_months):\n",
    "    seasonal_avg = np.where(df['center_lat'] >= 0, df[nh_months].mean(axis=1), df[sh_months].mean(axis=1))\n",
    "    return seasonal_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean lake surface water temperature for all 12 months\n",
    "lake_data['lswt_ann_avg'] = lake_data[['Tmonth_mean_1', 'Tmonth_mean_2', 'Tmonth_mean_3', 'Tmonth_mean_4', \n",
    "                             'Tmonth_mean_5', 'Tmonth_mean_6', 'Tmonth_mean_7', 'Tmonth_mean_8', \n",
    "                             'Tmonth_mean_9', 'Tmonth_mean_10', 'Tmonth_mean_11', 'Tmonth_mean_12']].mean(axis=1)\n",
    "\n",
    "\n",
    "# Calculate the seasonal average for NH April-October (A-O) and SH October-April (A-O)\n",
    "lake_data['lswt_ao_avg'] = calculate_seasonal_avg(lake_data, \n",
    "                                                   ['Tmonth_mean_4', 'Tmonth_mean_5', 'Tmonth_mean_6', 'Tmonth_mean_7', 'Tmonth_mean_8', 'Tmonth_mean_9', 'Tmonth_mean_10'],\n",
    "                                                   ['Tmonth_mean_10', 'Tmonth_mean_11', 'Tmonth_mean_12', 'Tmonth_mean_1', 'Tmonth_mean_2', 'Tmonth_mean_3', 'Tmonth_mean_4'])\n",
    "\n",
    "# Calculate the seasonal average for NH April-May-June (AMJ) and SH October-November-December (OND)\n",
    "lake_data['lswt_amj_avg'] = calculate_seasonal_avg(lake_data, \n",
    "                                         ['Tmonth_mean_4', 'Tmonth_mean_5', 'Tmonth_mean_6'],\n",
    "                                         ['Tmonth_mean_10', 'Tmonth_mean_11', 'Tmonth_mean_12'])\n",
    "\n",
    "# Calculate the seasonal average for NH June-July-August (JJA) and SH December-January-February (DJF)\n",
    "lake_data['lswt_jja_avg'] = calculate_seasonal_avg(lake_data, \n",
    "                                         ['Tmonth_mean_6', 'Tmonth_mean_7', 'Tmonth_mean_8'],\n",
    "                                         ['Tmonth_mean_12', 'Tmonth_mean_1', 'Tmonth_mean_2'])\n",
    "\n",
    "# Find the warmest month for each row and create a new column 'warmest_avg'\n",
    "lake_data['lswt_warmest_avg'] = lake_data[['Tmonth_mean_1', 'Tmonth_mean_2', 'Tmonth_mean_3', 'Tmonth_mean_4', \n",
    "                            'Tmonth_mean_5', 'Tmonth_mean_6', 'Tmonth_mean_7', 'Tmonth_mean_8', \n",
    "                            'Tmonth_mean_9', 'Tmonth_mean_10', 'Tmonth_mean_11', 'Tmonth_mean_12']].max(axis=1)\n",
    "\n",
    "#convert lake_data to Celcius\n",
    "lake_data['lswt_ann_avg']  = (lake_data['lswt_ann_avg']/100)\n",
    "lake_data['lswt_ao_avg']  = (lake_data['lswt_ao_avg']/100)\n",
    "lake_data['lswt_amj_avg']  = (lake_data['lswt_amj_avg']/100)\n",
    "lake_data['lswt_jja_avg']  = (lake_data['lswt_jja_avg']/100)\n",
    "lake_data['lswt_warmest_avg']  = (lake_data['lswt_warmest_avg']/100)\n",
    "\n",
    "\n",
    "# Print the first few rows to verify the new columns\n",
    "lake_data[['center_long', 'center_lat', 'lswt_ann_avg', 'lswt_ao_avg', 'lswt_amj_avg', 'lswt_jja_avg', 'lswt_warmest_avg']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column 'abs_lat' with the absolute values of 'center_lat'\n",
    "lake_data['abs_lat'] = abs(lake_data['center_lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14932bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column for seasonal LSWT^2\n",
    "lake_data['sq_lswt_ann'] = lake_data['lswt_ann_avg']**2\n",
    "lake_data['sq_lswt_ao'] = lake_data['lswt_ao_avg']**2\n",
    "lake_data['sq_lswt_amj'] = lake_data['lswt_amj_avg']**2\n",
    "lake_data['sq_lswt_jja'] = lake_data['lswt_jja_avg']**2\n",
    "lake_data['sq_lswt_warmest'] = lake_data['lswt_warmest_avg']**2\n",
    "\n",
    "# Transform lake area and depth\n",
    "lake_data['log_lake_area'] = np.log(lake_data['Lake_area'])\n",
    "lake_data['log_depth'] = np.log(lake_data['Depth_avg'])\n",
    "\n",
    "# Covert elevation from meters to km\n",
    "lake_data['elevation_km'] = lake_data['Elevation']/1000\n",
    "\n",
    "\n",
    "lake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d328f9c",
   "metadata": {},
   "source": [
    "## Find nearest values at the LakeTemp lake coordinates for variables: tas, tcc, rh, and u10 and caculate seasonal and mean annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify the coordinate range\n",
    "# print(f\"Lake data longitude range: {lake_data['center_long'].min()} to {lake_data['center_long'].max()}\")\n",
    "# print(f\"Lake data latitude range: {lake_data['center_lat'].min()} to {lake_data['center_lat'].max()}\")\n",
    "\n",
    "# print(f\"ERA5 data longitude range: {tas_monthly_mean.coords['longitude'].min().values} to {tas_monthly_mean.coords['longitude'].max().values}\")\n",
    "# print(f\"ERA5 data latitude range: {tas_monthly_mean.coords['latitude'].min().values} to {tas_monthly_mean.coords['latitude'].max().values}\")\n",
    "\n",
    "# Load the datasets (assuming they are already loaded as tas_monthly_mean, tcc_monthly_mean, RH_monthly_mean, u10_monthly_mean)\n",
    "tas_monthly_mean.load()\n",
    "tcc_monthly_mean.load()\n",
    "RH_monthly_mean.load()\n",
    "ssr_monthly_mean.load()\n",
    "u10_monthly_mean.load()\n",
    "\n",
    "# Prepare the coordinates from the LakeTemp data DataFrame\n",
    "coordinates = lake_data[['center_long', 'center_lat']].values\n",
    "\n",
    "# Initialize dictionaries to store the nearest values for each month for all datasets\n",
    "nearest_values = {var: {month: [] for month in range(1, 13)} for var in ['temp', 'tcc', 'rh', 'ssr', 'u10']}\n",
    "\n",
    "print(\"Initialized nearest values dictionaries\")\n",
    "\n",
    "# Convert the coordinates to an xarray DataArray for vectorized operations\n",
    "coords_da = xr.DataArray(coordinates, dims=['points', 'coord'], coords={'coord': ['longitude', 'latitude']})\n",
    "\n",
    "# Loop through each month and find the nearest values for all datasets at once\n",
    "for month in range(1, 13):\n",
    "    nearest_values['temp'][month] = tas_monthly_mean.sel(month=month).sel(longitude=coords_da.sel(coord='longitude'), \n",
    "                                                                          latitude=coords_da.sel(coord='latitude'), \n",
    "                                                                          method='nearest').values\n",
    "    nearest_values['tcc'][month] = tcc_monthly_mean.sel(month=month).sel(longitude=coords_da.sel(coord='longitude'), \n",
    "                                                                         latitude=coords_da.sel(coord='latitude'), \n",
    "                                                                         method='nearest').values\n",
    "    nearest_values['rh'][month] = RH_monthly_mean.sel(month=month).sel(longitude=coords_da.sel(coord='longitude'), \n",
    "                                                                       latitude=coords_da.sel(coord='latitude'), \n",
    "                                                                       method='nearest').values\n",
    "    nearest_values['ssr'][month] = ssr_monthly_mean.sel(month=month).sel(longitude=coords_da.sel(coord='longitude'), \n",
    "                                                                         latitude=coords_da.sel(coord='latitude'), \n",
    "                                                                         method='nearest').values\n",
    "    nearest_values['u10'][month] = u10_monthly_mean.sel(month=month).sel(longitude=coords_da.sel(coord='longitude'), \n",
    "                                                                         latitude=coords_da.sel(coord='latitude'), \n",
    "                                                                         method='nearest').values\n",
    "    print(f\"Month {month}: Processed nearest values for all datasets\")\n",
    "\n",
    "# Add the lists of nearest values as columns to the DataFrame\n",
    "for var in ['temp', 'tcc', 'rh', 'ssr', 'u10']:\n",
    "    for month in range(1, 13):\n",
    "        lake_data[f'nearest_{var}_{month}'] = nearest_values[var][month]\n",
    "\n",
    "# Drop NaN values in each of the nearest variables\n",
    "for month in range(1, 13):\n",
    "    lake_data = lake_data.dropna(subset=[f'nearest_temp_{month}', f'nearest_tcc_{month}', f'nearest_rh_{month}', f'nearest_ssr_{month}', f'nearest_u10_{month}'])\n",
    "\n",
    "\n",
    "# Calculate the mean temperature for all 12 months\n",
    "lake_data['tas_ann_avg'] = lake_data[[f'nearest_temp_{month}' for month in range(1, 13)]].mean(axis=1)\n",
    "\n",
    "# Calculate the seasonal average for NH April-October (A-O) and SH October-April (A-O)\n",
    "lake_data['tas_ao_avg'] = calculate_seasonal_avg(lake_data, \n",
    "                                                 [f'nearest_temp_{month}' for month in [4, 5, 6, 7, 8, 9, 10]],\n",
    "                                                 [f'nearest_temp_{month}' for month in [10, 11, 12, 1, 2, 3, 4]])\n",
    "\n",
    "# Calculate the seasonal average for NH April-May-June (AMJ) and SH October-November-December (OND)\n",
    "lake_data['tas_amj_avg'] = calculate_seasonal_avg(lake_data, \n",
    "                                                  [f'nearest_temp_{month}' for month in [4, 5, 6]],\n",
    "                                                  [f'nearest_temp_{month}' for month in [10, 11, 12]])\n",
    "\n",
    "# Calculate the seasonal average for NH June-July-August (JJA) and SH December-January-February (DJF)\n",
    "lake_data['tas_jja_avg'] = calculate_seasonal_avg(lake_data, \n",
    "                                                  [f'nearest_temp_{month}' for month in [6, 7, 8]],\n",
    "                                                  [f'nearest_temp_{month}' for month in [12, 1, 2]])\n",
    "\n",
    "# Find the warmest month for each row and create a new column 'tas_warmest_month'\n",
    "lake_data['tas_warmest_month'] = lake_data[[f'nearest_temp_{month}' for month in range(1, 13)]].max(axis=1)\n",
    "\n",
    "# Calculate the mean values for TCC, RH, and U10 for all 12 months\n",
    "for var in ['tcc', 'rh', 'ssr', 'u10']:\n",
    "    lake_data[f'{var}_ann_avg'] = lake_data[[f'nearest_{var}_{month}' for month in range(1, 13)]].mean(axis=1)\n",
    "\n",
    "# Print the first few rows to verify the new columns\n",
    "lake_data[['center_long', 'center_lat', \n",
    "                 'tas_ann_avg', 'tas_ao_avg', 'tas_amj_avg', 'tas_jja_avg', 'tas_warmest_month',\n",
    "                 'tcc_ann_avg', 'rh_ann_avg', 'ssr_ann_avg', 'u10_ann_avg']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d85a24",
   "metadata": {},
   "source": [
    "## Save lake_data df as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_data.to_csv('ERA5_LakeTemp.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pliomip2)",
   "language": "python",
   "name": "pliomip2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
